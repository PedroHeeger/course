# DevOps   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/content/devops.png" alt="devops" width="auto" height="45">

### Repository: [course](../../../../)
### Platform: <a href="../../">fabricio_veronez   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/plataforma/fabricio_veronez.png" alt="fabricio_veronez" width="auto" height="25"></a>
### Software/Subject: <a href="../">devops   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/content/devops.png" alt="devops" width="auto" height="25"></a>
### Course: <a href="./">curso_081 (Imersão DevOps & Cloud)   <img src="./curso_081/0-aux/logo_course.png" alt="curso_081" width="auto" height="25"></a>

#### <a href="">Certificate</a>

---

### Theme:
- Cloud Computing
- DevOps

### Used Tools:
- Operating System (OS): 
  - Linux   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/linux/linux-original.svg" alt="linux" width="auto" height="25">
  - Windows 11   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/windows11.png" alt="windows11" width="auto" height="25">
- Linux Distribution: 
  - Ubuntu   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/ubuntu/ubuntu-plain.svg" alt="ubuntu" width="auto" height="25">
- Virtualization: 
  - Docker   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/docker/docker-original.svg" alt="docker" width="auto" height="25">
- Cloud:
  - Amazon Web Services (AWS)   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/amazonwebservices/amazonwebservices-original.svg" alt="aws" width="auto" height="25">
- Cloud Services:
  - Amazon Elastic Compute Cloud (EC2)   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/cloud/aws_ec2.svg" alt="aws_ec2" width="auto" height="25">
  - Google Drive <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/google_drive.png" alt="google_drive" width="auto" height="25">
- Cluster Management Software:
  - Kubernetes   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/kubernetes/kubernetes-plain.svg" alt="kubernetes" width="auto" height="25">
  - K3D   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/kubernetes_k3d.png" alt="kubernetes_k3d" width="auto" height="25">
- Configuration Management (CM):
  - Terraform   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/terraform/terraform-original.svg" alt="terraform" width="auto" height="25">
- Language:
  - HTML   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/html5/html5-original.svg" alt="html" width="auto" height="25">
  - Markdown   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/markdown/markdown-original.svg" alt="markdown" width="auto" height="25">
- BI Tool:
  - Grafana   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/grafana/grafana-original.svg" alt="grafana" width="auto" height="25">
- Integrated Development Environment (IDE) and Text Editor:
  - Visual Studio Code (VS Code)   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/vscode/vscode-original.svg" alt="vscode" width="auto" height="25">
- Versioning: 
  - Git   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/git/git-original.svg" alt="git" width="auto" height="25">
- Repository:
  - Docker Hub   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/docker_hub.png" alt="docker_hub" width="auto" height="25">
  - Docker Registry   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/docker_registry.png" alt="docker_registry" width="auto" height="25">
  - GitHub   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/github/github-original.svg" alt="github" width="auto" height="25">
  - Terraform Registry   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/terraform/terraform-original.svg" alt="terraform_registry" width="auto" height="25">
- Command Line Interpreter (CLI):
  - AWS Command Line Interface (CLI)   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/cloud/aws_cli.svg" alt="aws_cli" width="auto" height="25">
  - Azure CLI   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/azure/azure-original.svg" alt="azure_cli" width="auto" height="25">
  - Bash e Sh   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/bash/bash-original.svg" alt="bash_sh" width="auto" height="25">
  - Oh My Zshell (Oh My ZSh)   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/oh_my_zshell.png" alt="oh_my_zshell" width="auto" height="25">
  - Systemctl   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/systemctl.png" alt="systemctl" width="auto" height="25">
  - Windows PowerShell   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/windows_power_shell.png" alt="windows_power_shell" width="auto" height="25">
  - ZShell   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/zshell.png" alt="zshell" width="auto" height="25">
- Server and Databases:
  - Apache HTTP Server (httpd)   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/apache_http_server.png" alt="apache_httpd" width="auto" height="25">
  - Prometheus   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/prometheus/prometheus-original.svg" alt="prometheus" width="auto" height="25">
- Workflow Management System (WFMS):
  - GitHub Actions   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/github_actions.png" alt="github_actions" width="auto" height="25">
  
---

<a name="item0"><h3>Course Strcuture:</h3></a>
1. <a href="#item01">Aula 1 - Revolução Digital com DevOps e Cloud</a><br>
2. <a href="#item02">Aula 2 - Kubernetes do zero ao deploy</a><br>
3. <a href="#item03">Aula 3 - AWS: Potencialize sua aplicação com o poder da Cloud Computing</a><br>
4. <a href="#item04">Aula 4 - Github Actions - Eficiência em entregas automatizadas</a><br>
5. <a href="#item05">Class 05</a><br>

---

### Objective:
O objetivo desse projeto prático foi introduzir algumas das principais ferramentas da área de **DevOps**, que são elas: **Docker**, **Kubernetes**, **AWS**, **GitHub Actions** e **Terraform**. Neste projeto foi desenvolvido uma aplicação web de um microblog (portal de notícias) escrito em **Node.js** que tem como persistência da base de dados no banco **PostgreSQL**. Também foi desenvolvido um pipeline de entrega e integração contínua deste projeto utilizando os softwares **GitHub Actions**, **Kubernetes** como ambiente de execução da aplicação, o **Docker** como tecnologia para empacotar a imagem de container para rodar essa aplicação, e o monitoramento dessa aplicação com o **Grafana** e o **Prometheus**.

### Structure:
A estrutura (Imagem 01) do projeto é formada por:
- Dois arquivos em **Excel** um para cada aula, sendo as aulas 2 e 3 resolvidas no mesmo arquivo. 
- Uma pasta com as imagens dos ícones inseridos no report. 
- Duas pastas, uma contendo três arquivos de base de dados em **Excel**, um para cada ano (2021, 2022, 2023), e a outra contendo, também em um arquivo de **Excel**, a base de dados só do mês de Julho/2023 para ser inserida posteriormente. 
- A pasta **0-aux**, pasta auxiliar com imagens utilizadas na construção desse arquivo de README. 
- Obs.: A logomarca do curso foi criada apenas para fins didáticos com uso do site de inteligência artificial **Looka**.

<div align="Center"><figure>
    <img src="./0-aux/img01.PNG" alt="img01"><br>
    <figcaption>Imagem 01.</figcaption>
</figure></div><br>

### Development:
Este projeto foi desenvolvido em cinco aulas, além de conter quatro lives e três desafios. As etapas do projeto estão listadas abaixo.

- Empacotar a aplicação em imagens Docker para executar em containers.
- Rodar a aplicação em ambiente Kubernetes local usando K3D e ambiente de nuvem com AWS.
- Automatizar a entrega das aplicações usando pipeline CI/CD com GitHub Actions.
- Utilizar infraestrutura como código para criar a infraestrutura pra conseguir agilidade, reaproveitamento e confiabilidade.
- Monitorar a aplicação e a infraestrutura para saber o que está acontencendo e se antecipar aos problemas.

<a name="item01"><h4>Aula 1 - Revolução Digital com DevOps e Cloud</h4></a>[Back to summary](#item0)

Na primeira aula desse curso, foi realizada uma introdução sobre o software **Docker**, mostrando alguns comandos básicos desta ferramenta. Para execução do projeto, foi decidido por mim que tudo seria executado na cloud da **AWS** com o objetivo de evitar realizar instalações na maquina física, utilizando o **AWS CLI** no **PowerShell**. Portanto, foi construída e configurada uma maquina virtual **Linux Ubuntu** no serviço **Amazon EC2** da cloud para servir como ambiente de execução, onde seriam feitas as instalações dos programas utilizados e o download dos arquivos do projeto. Todo o processo de configuração desse ambiente foi realizado de forma automatizada através dos três arquivos seguintes de **PowerShell**: [criacao](./automation/criacao.ps1), [exclusao](./automation/exclusao.ps1) e [variaveis](./automation/variaveis.ps1), sendo todos eles armazenados no diretório [automation](./automation/). Este diretório ainda conteve duas sub-pastas, a primeira ([resources](./automation/resources/)), para armazenar os arquivos de recursos necessários, que neste caso, armazenou o arquivo de script em **Bash** [ec2Script.sh](./automation/resources/ec2Script.sh). A outra sub-pasta de nome [secrets](./automation/secrets) continha as credenciais para login do usuário na **AWS CLI**, no **Docker Hub** e o arquivo par de chaves `.pem` gerado para realização de acesso remoto na maquina virtual instanciada na cloud da **AWS**.

No script de criação, todo comando executado foi precedido por comandos de verificação determinando se o serviço ou recurso já havia sido criado através estruturas de condicionais `if else`. Caso o resultado fosse que o elemento já tinha sido criado, o nome dele era listado, além de informações necessárias que também eram exibidas. Já se não houvesse sido criado, era listado o antes e o depois do comando de criação para evidenciar a construção do serviço. Nesta segunda situação, também era listada informações adicionais quando necessário.

O script iniciou com a execução de um par de chaves na cloud da **AWS**, onde as informações foram também armazenadas em um arquivo formato `.pem` dentro da sub-pasta `secrets`, para utilização durante acesso remoto a maquina. Em seguida, foi instanciada a maquina virtual no serviço **EC2** que funcionaria como ambiente de execução, indicando um arquivo de script **Bash** ([ec2Script.sh](./automation/resources/ec2Script.sh)) que seria responsável por realizar as instalações dos softwares e execução dos comandos do projeto. Após isso, uma regra foi adicionada ao grupo de segurança padrão da VPC padrão, ambos utilizados na maquina virtual instanciada. Esta regra consistiu na liberação da porta `8080` do protocolo `TCP` para todas as faixas de IP, para que posteriormente fosse possível acessar a aplicação por um navegador da web na maquina física.

Dando seguimente, foi aguardado um cerca de 200 segundos para que uma parte do script em **Bash** fosse executada, alguns softwares fossem baixados e também que fosse feito o clone da pasta do projeto do **GitHub**, pois alguns arquivos que seriam transferidos iriam para sub-pastas da pasta do projeto. Em seguida foi exibido o endereço para acesso a aplicação que era o IP público da maquina virtual concatenado com o número da porta, que no caso foi a `8080`. Este número de IP passou por um processo de conversão de caracteres para trocar o ponto que separava os números do IP para um traço e então o comando para acesso remoto a maquina era exibido na tela de modo que só era necessário copiar, colar e executar em um shell, que no caso foi utilizado no **PowerShell** da maquina físca. 

Cerca de 15 segundos foram aguardados para realização do acesso remoto, onde nesse momento o script em **Bash** estaria terminando de instalar e configurar o software **Docker**. Neste momento através do software **OpenSSH**, o mesmo utilizado para acesso remoto, foram realizadas verificações dentro da maquina virtual da cloud para determinar se arquivos e pastas que deveriam ser enviadas já existiam nos seus respectivos locais do sistema de arquivos, caso esses arquivos não fossem encontrados, eram enviados para a maquina. Os arquivos transferidos foram: a pasta `.aws` contendo os arquivos `credentials` e `config` para configuração do usuário administrador `PedroheegerAdmin` no **AWS CLI** da maquina virtual, na maquina física isso já era configurado; a pasta `.docker` contendo o arquivo `config.json` que configura o acesso ao repositório do **Docker** (**Docker Hub**) através de um usuário; os arquivos `deployment1.yaml` e `deployment2.yaml` para execução de dois projetos, um projeto de teste do software **Kubernetes** e o projeto principal; e também o arquivo `Dockerfile` para construção da imagem do projeto principal.

Enquanto o script de criação era executado, o script **Bash** também era executado logo após a maquina virtual está instanciada. Este script foi dividido em etapas e todos os comandos e etapas executados precedem por um comando de `echo` para exibir a ação que foi realizada e em que parte do projeto isso estava. A execução desses comandos podem ser visualizadas através do arquivo `/var/log/cloud-init-output.log` dentro da maquina virtual. Todos os projetos preeliminares realizados foram executados em sequencial com o projeto principal, ou seja, a medida que o script estava em execução um projeto menor era realizado, em seguida ele era excluído e o próximo projeto era executado, sendo uma execução sequenciada e praticamente toda automatizada.

A primeira parte do script em **Bash** que rodou dentro da instância executou instalações básicas que já era padrão dos projetos que tenho criado em outros cursos ou bootcamps. Porém antes dessas intalações, o diretório corrente foi alterado para o diretório do usuário `/home/ubuntu`, sendo tudo instalado e baixado dentro deste diretório. Também foi feita uma atualização dos pacotes e do sistema e então os softwares básicos (**Nano** **Wget**, **Curl**, **Git**) foram instalado. Eles eram necessários para manipulação dentro do **Linux Ubuntu**. Também foi instalado o **ZShell**, um software de interface de linha de comando (CLI) que gosto de utilizar no **Linux**, ele foi definido como shell padrão da maquina instanciada e os três seguintes plugins dele foram baixados e configurados: **powerlevel10k**, **zsh-autosuggestions** e **zsh-syntax-highlighting**. O software de CLI da **AWS** também foi baixado, onde não foi necessário fazer o login do usuário, pois a pasta `.aws` com os arquivos enviados da maquina física se encarregaram de fazer essa configuração.

Na segunda etapa desse script foi feito o clone do repositório do projeto do **GitHub** fornecido pelo professor cujo nome da pasta do projeto foi `imersao-devops-cloud-02` e teve seu proprietário e grupo alterado de usuário `root` para o usuário `ubuntu`, usuário que realizava o acesso remoto. Em seguida foi feita a instalação do software **Docker** e a adição do usuário `ubuntu` ao grupo do **Docker** para executar os comandos sem a utilização do `sudo`. Antes de partir para etapa três, foi aguardado cerca de 100 segundos para esperar todos os cinco arquivos da maquina física serem transferidos para a instância na cloud.

A etapa 3 compreende extamente o desenvolvido na aula 1 deste curso, que foi um projeto preeliminar para aprendizagem do software **Docker**, cujo nome era `conversao-temperatura`. Ele foi inciado com o acesso a pasta deste projeto (`/home/ubuntu/imersao-devops-cloud-02/conversao-temperatura/src`) onde já existia um arquivo `Dockerfile` e os arquivos da aplicação clonados do repositório do **GitHub**. Nesta pasta foi elaborado um arquivo `.dockerignore` para ignorar a pasta do `node_modules` que era criada e então foi feito o build da imagem **Docker**. Essa imagem foi tagueda para as versões `v1` e `latest` sendo ambas enviadas para o **Docker Hub** (um repositório de imagens **Docker**) e podendo ser baixadas de qualquer lugar e por qualquer pessoa com acesso a internet e conta no **Docker Hub**. Por fim, foi executado o comando (`docker container run --name aplicacao1 -d -p 8080:8080 conversao-temperatura`) para criação do container da aplicação a partir dessa imagem construída pelo arquivo `Dockerfile` e realizando um bind de portas, ou seja, combinando a porta `8080` da maquina instanciada com a porta `8080` do container. Assim a aplicação pode ser acessada e executada através do número de IP público da maquina virtual concatenado com o número da porta `:8080`. 

A imagem 02 a seguir mostra a aplicação sendo acessada pelo navegador de internet da maquina física no IP da instância e na porta estabelecida. Agora fica explicado o porque foi nessário criar uma regra para a liberação da porta `8080` da maquina virtual da cloud. Após a imagem, o script do arquivo `Dockerfile` é exibido, observe que foi utilizado uma imagem base do **Node.js**, os arquivos `package` em formato **JSON** foram copiados e com a execução do comando `npm install`, as dependências nesses arquivos `package` eram criadas no sub-diretório `node_modules` que também era criado. Por fim, os demais arquivos da aplicação eram copiados, a aplicação ficava exposta na porta `8080` do container e o comando `node server.js` era acionado para execução da aplicação. Enquanto este container estivesse ativo, a aplicação estaria sendo executada no endereço estabelecido. A aplicação consistiu em um conversor de temperatura que convertia as temperaturas em Fahrenheit para Celsius e Celsius para Fahrenheit.

<div align="Center"><figure>
    <img src="./0-aux/img02.PNG" alt="img02"><br>
    <figcaption>Imagem 02.</figcaption>
</figure></div><br>

```dockerfile
FROM node:18.16.0
WORKDIR /app
COPY package*.json ./
RUN npm install
COPY . .
EXPOSE 8080
CMD ["node", "server.js"]
```

kubectl create -f deployment.yaml
127
128
193
<a name="item02"><h4>Aula 2 - Kubernetes do zero ao deploy</h4></a>[Back to summary](#item0)

As próximas três etapas (4, 5 e 6) do arquivo de script em **Bash** são referentes a aula 2 deste curso. A etapa 4 consistiu na instalação do softwares **k3d** para execução de clusters **Kubernetes** e o **kubectl** que é um CLI para **Kubernetes**. Ao terminar de baixar o **kubectl** foi necessário alterar o proprietário e grupo da pasta `.kube` bem como do arquivo de configuração dentro desta pasta `.kube/config`. Cerca de 100 segundos foram aguardados antes de iniciar a etapa 5, neste momento o projeto 1 (`conversao-temperatura`) ainda estava no ar.

A etapa cinco iniciou com a remoção do container da aplicação do projeto 1 e então a pasta principal do projeto (`imersao-devops-cloud-02`) foi acessada. Neste diretório, um segundo projeto foi criado, assim como o primeiro foi um projeto preeliminar, agora para introduzir o software **Kubernetes**. Neste projeto foi criado um cluster com o comando `k3d cluster create meucluster1 -p "8080:30000@loadbalancer"` e em seguida executado o primeiro arquivo de manifesto **YAML** ([deployment1.yaml](./automation/resources/deployment1.yaml)) que foi enviado da maquina física para instância. O comando utilizado para execução foi o `kubectl apply -f deployment1.yaml`. Aproximadamente 100 segundos foram aguardados antes de realizar uma alteração na aplicação. No arquivo **YAML** `deployment1.yaml`, existia uma aplicação e um serviço, a imagem utilizada pelo container da aplicação era `fabricioveronez/web-page:blue` que era baixado do repositório do professor no **Docker Hub**. Porém, agora, essa imagem foi alterada para `fabricioveronez/web-page:green`. A diferença entre uma e outra era apenas a cor de fundo da aplicação. Então um novo deploy do arquivo **YAML** foi feito para a trocar da imagem do container da aplicação e cerca de 100 segundos foram aguardados com essa aplicação rodando. Por fim, a aplicação foi derrubada com o comando `kubectl delete -f deployment1.yaml` e o cluster removido com o comando `k3d cluster delete meucluster1`, finalizando o segundo projeto. As imagens 3 e 4 exibem a aplicação no ar através do navegador da web, sendo uma com a imagem cujo fundo da aplicação é azul e a outra com o fundo verde. Mais 45 segundos foram aguardados para concluir a remoção do cluster.

<div align="Center"><figure>
    <img src="./0-aux/img03.PNG" alt="img03"><br>
    <figcaption>Imagem 03.</figcaption>
</figure></div><br>

<div align="Center"><figure>
    <img src="./0-aux/img04.PNG" alt="img04"><br>
    <figcaption>Imagem 04.</figcaption>
</figure></div><br>

Na etapa 6 foi acessada a pasta do projeto principal `/home/ubuntu/imersao-devops-cloud-02/kube-news/src`, cujo nome do projeto era `kube-news`. Neste diretório, também foi criado um arquivo `.dockerignore`, ignorando a pasta `node_modules`, como visto anteriormente essa pasta é criada ao executar o comando `npm install`. Com o arquivo `Dockerfile` enviado da maquina física para a instância no sub-diretório `src`, o build da imagem foi realizado cujo nome ficou `pedroheeger/curso081_kube-news:v1`. Uma versão `latest` também foi tagueada a partir da versão `v1`. Ambas foram enviadas para um repositório no **Docker Hub**. Com a imagem criada, o cluster foi construído com o comando `k3d cluster create meucluster2 -p "8080:30000@loadbalancer"`.

O segundo arquivo de manifesto foi transferido da maquina física para a instância, criando uma sub-pasta `k8s` dentro do diretório do projeto principal `kube-news`. Alterando para esse sub-diretório, o comando de execução do manifesto foi realizado (`kubectl apply -f deployment2.yaml`). Nesse momento a aplicação já estava rodando na porta `8080`. Essa aplicação consistiu em um blog, porém este blog estava sem dados e para inserir dados no blog foi utilizado o arquivo [popula-dados.http](./automation/resources/popula-dados.http). Nele, foi alterado na URL `http://localhost:8080/api/post`, o `localhost` pelo IP da maquina da instância e com esse arquivo aberto no **Visual Studio Code (VS Code)**, utilizando a extensão **REST Client** foi clicado na opção `Send Request` para enviar uma requisição do tipo `POST` para endereço do blog onde os dados desse arquivo seriam inseridos como conteúdo do blog. Cerca de 150 segundos foram aguardados para garantir que o blog fosse populado com dados.

Agora, foi o momento de realizar a mudança de versão, ou seja, foi realizada uma alteração em no arquivo da aplicação (`/home/ubuntu/imersao-devops-cloud-02/kube-news/src/views/partial/header.ejs`), onde na linha 4 deste arquivo foi inserido `- v2` após o comando `<img class="logo" src="/img/kubenews-logo.svg" alt="Kubenews" srcset="" />`. A pasta corrente era a `k8s`, então foi necessário alterar para a pasta do arquivo `DockerFile` (`/home/ubuntu/imersao-devops-cloud-02/kube-news/src`) e fazer o build da imagem na versão `v2` e subi-lá para o **Docker Hub**. Voltando para a pasta `k8s`, foi feita uma alteração também no arquivo de manifesto **YAML** (`deployment2.yaml`), alterando a imagem do container da aplicação de `v1` para `v2`. As alterações foram aplicadas executando o arquivo de manifesto `kubectl apply -f deployment2.yaml`. Por fim, após cerca de 150 segundos a aplicação foi removida e o cluster excluído.

Na imagem 05 é exibido a aplicação sem a população de dados. Já nas imagens 06 e 07 é exibido a aplicação populada com dados nas versões `v1` e  `v2` respectivamente.

<div align="Center"><figure>
    <img src="./0-aux/img05.PNG" alt="img05"><br>
    <figcaption>Imagem 05.</figcaption>
</figure></div><br>

<div align="Center"><figure>
    <img src="./0-aux/img06.PNG" alt="img06"><br>
    <figcaption>Imagem 06.</figcaption>
</figure></div><br>

<div align="Center"><figure>
    <img src="./0-aux/img07.PNG" alt="img07"><br>
    <figcaption>Imagem 07.</figcaption>
</figure></div><br>




10250
6443


<a name="item03"><h4>Aula 3 - AWS: Potencialize sua aplicação com o poder da Cloud Computing</h4></a>[Back to summary](#item0)

A aula três foi iniciada com a criação de uma conta na **AWS**, e de um usuário administrador (`PedroheegerAdmin`) para esta conta, anexando a ele a permissão `AdministratorAccess`. Também foi necessário baixar, instalar e configurar este usuário no **AWS CLI** que foi usado no **PowerShell** da maquina física para executar comandos na cloud da **AWS**. Todo esse processo preeliminar já havia sido feitos devido a outros cursos já realizados. 

Esta aula possuiu duas etapas, sendo que elas gerariam custos na **AWS**. Então foi necessário acessar o console da **AWS** com o usuário root e configurar um alerta através do serviço **AWS Budget** para receber uma notificação por email quando o custos do serviços atingisse 50% do valor determinado. Este processo teve que ser feito diretamente na conta do usuário root e pelo console, pois na **AWS CLI** instalada na maquina física, o usuário configurado era o usuário administrador que não tinha permissões relacionadas a finanças.

A primeira etapa desta aula foi a construção de uma pequena infraestrutura de rede para entender o seu funcionamento. Através do script de criação foram criados no serviço **AWS VPC**, uma VPC com duas sub-redes, sendo uma pública e outra privada. Também foram criados um Internet Gateway para liberar o acesso a internet para a sub-rede pública e um NAT Gateway para liberar o acesso a internet de dentro para fora da sub-rede privada. Para o NAT Gateway foi necessário alocar um IP Elástico aleatório. Foram construídas duas tabelas de rotas, sendo também uma pública e uma privada, cada uma com uma rota criada vinculando a sua sub-rede com o Internet Gateway para rede pública, e o NAT Gateway para rede privada.

Ao criar a VPC, automaticamente um grupo de segurança padrão para esta VPC foi gerado. Então ele foi utilizado, recebendo uma tag de nome e criando três regras de entrada para liberação das portas `8080`, `443` e `22` no protocolo `TCP` para todos os IPs. A porta `8080` e `443` eram as portas para acesso ao servidor **Nginx** que seria construído. Já a porta `22` era para acesso remoto as instâncias na cloud via software **OpenSSH** utilizado no **PowerShell** da maquina física **Windows**.

Em seguida, duas instâncias **Linux Ubuntu** foram criadas no serviço **Amazon EC2** do tipo `t2.micro`, sendo uma vinculada a sub-rede pública e outra a sub-rede privada. Em amabas foi indicado que o arquivo de script em **Bash** [ec2Script2.sh](./automation/resources/ec2Script2.sh) deveria ser executado quando as maquinas estivessem prontas. Este script apenas atualizou os pacotes e o sistema e fez a instalação dos softwares: **Nano**, **Wget**, **Curl** e o **Nginx**. Na instância vinculada a sub-rede pública o acesso remoto era feito com o comando `ssh -i "$keyPairPath\$keyPairName.pem" ubuntu@ec2-$ipEc2.compute-1.amazonaws.com` ou `ssh -i "$keyPairPath\$keyPairName.pem" ubuntu@$ipEc2`, sendo na primeira opção o IP deve ser separados por traços (`-`) e na segunda separado da forma normal por pontos (`.`). 

O par de chave gerado na cloud **AWS** e o arquivo `.pem` foram os mesmos dos desenvolvidos na Aula 1 e 2, onde só seriam necessário gerar de novo se tivesse sido excluído, sendo que para gerar era só executar o bloco de código destinado a criação do par de chaves. Lembrando que o arquivo par de chaves `.pem` ficou armazenado na sub-pasta `secrets`, pasta essa que não é versionada para o **GitHub** por ter dados sensíveis. Este arquivo foi enviado da maquina física **Windows** para a instância pública na cloud **AWS** via **OpenSSH** através do comando `scp -i "$keyPairPath\$keyPairName.pem" -o StrictHostKeyChecking=no -r "$keyPairPath\$keyPairName.pem" ubuntu@${ipEc2}:/home/ubuntu/.ssh` para que fosse possível acessar remotamente a maquina privada com a instância pública, logo a instância privada seria um **Jump Server**. Este segundo acesso remoto utilizou o IP privado da instância e não o IP público como de costume. Este acesso só foi possível, pois as duas sub-redes onde estava cada uma dessas instâncias eram partes da mesma rede. A imagem 19 abaixo mostra o arquivo par de chaves `.pem` já na instância pública dentro do diretório `.ssh` e o funcinamento do Jump Server, ou seja acessando remotamente a maquina privada pela maquina pública.

<div align="Center"><figure>
    <img src="./0-aux/img19.PNG" alt="img19"><br>
    <figcaption>Imagem 19.</figcaption>
</figure></div><br>

Como foi instalado o software **Nginx** nas duas maquinas, foi possível acessar este servidor web através do navegador de internet da maquina física, utilizando como URL, apenas IP público da instância concatenado com a porta `:80`, que poderia ser omitida. A imagem 20 a seguir mostra a realização dessa etapa apenas na instância pública. Na instância privada, o acesso não era possível pelo navegador, tanto no IP público como no privado, como mostrado nas imagens 21 e 22. Neste caso, para ver o funcionamento do **Nginx** foi necessário fazer o acesso remoto normal a instância pública pelo seu IP público no **PowerShell** e nela utilizar o software **Curl** para enviar uma requisição ao **Nginx** da maquina privada. A URL utilizada pelo **Curl** seria praticamente a mesma da utilizada do navegador, porém o IP da maquina que no navegador era público da instância pública, agora seria o IP privado da instância privada, mantendo a mesma porta `:80`, o comando utilizado foi o seguinte `curl -IL https://IP:80`. Na imagem 23 é possível visualizar o status de retorno de três requisições como sucedido feita pelo **Curl** dentro da instância pública. A primeira e segunda requisição são as mesmas, onde o IP público da instância pública é igual ao `localhost`, que é o IP público da própria maquina, que no caso era a instância pública. Já a terceira requisição era para o IP privado da maquina privada.

<div align="Center"><figure>
    <img src="./0-aux/img20.PNG" alt="img20"><br>
    <figcaption>Imagem 20.</figcaption>
</figure></div><br>

<div align="Center"><figure>
    <img src="./0-aux/img21.PNG" alt="img21"><br>
    <figcaption>Imagem 21.</figcaption>
</figure></div><br>

<div align="Center"><figure>
    <img src="./0-aux/img22.PNG" alt="img22"><br>
    <figcaption>Imagem 22.</figcaption>
</figure></div><br>

<div align="Center"><figure>
    <img src="./0-aux/img23.PNG" alt="img23"><br>
    <figcaption>Imagem 23.</figcaption>
</figure></div><br>

Para ficar um pouco melhor de entender, na maquina pública, o IP Público era `54.210.244.172` e o IP privado era `10.0.0.35`, já na maquina privada, o IP público era `54.173.6.213` e o IP privado era `10.0.1.147`. Observe que os IPs privados da instâncias obedeceram as faixas de IPs determinadas nas sub-redes construídas da VPC. Como pode ver, na imagem 24 é exibido as sub-redes, sendo a pública com IP `10.0.0.0/24` e a privada com IP `10.0.1.0/24`.

<div align="Center"><figure>
    <img src="./0-aux/img24.PNG" alt="img24"><br>
    <figcaption>Imagem 24.</figcaption>
</figure></div><br>

Nas imagens 25, 26, 27, 28 e 29 são evidenciados a criação do seguintes recursos: **Internet Gateway**, **NAT Gateway**, duas **Route Tables**, **IP Elástico** usado pelo NAT Gateway e o **Security Group**, onde foram definidas as regras de Firewall.

<div align="Center"><figure>
    <img src="./0-aux/img25.PNG" alt="img25"><br>
    <figcaption>Imagem 25.</figcaption>
</figure></div><br>

<div align="Center"><figure>
    <img src="./0-aux/img26.PNG" alt="img26"><br>
    <figcaption>Imagem 26.</figcaption>
</figure></div><br>

<div align="Center"><figure>
    <img src="./0-aux/img27.PNG" alt="img27"><br>
    <figcaption>Imagem 27.</figcaption>
</figure></div><br>

<div align="Center"><figure>
    <img src="./0-aux/img28.PNG" alt="img28"><br>
    <figcaption>Imagem 28.</figcaption>
</figure></div><br>

<div align="Center"><figure>
    <img src="./0-aux/img29.PNG" alt="img29"><br>
    <figcaption>Imagem 29.</figcaption>
</figure></div><br>

A primeira etapa desta aula 3 encerra aqui, com o script de exclusão tudo que foi construído foi removido, essa etapa gerou alguns gastos relacionados aos recursos do **NAT Gateway** e **IP Elástico** utilizados.

Na segunda etapa da aula, o projeto `kube-news` da segunda etapa da aula 2 foi executado em ambiente de cloud utilizando o serviço **Amazon EKS** para executar o cluster **Kubernetes**. Para essa etapa, além das configurações padrões de criação de conta e usuário administrador na **AWS**, instalação e conexão da **AWS CLI** com o usuário administrador, criação dos alertas na **AWS Budget**, também foi necessário baixar, instalar e configurar o **Kubectl** na maquina física **Windows** e baixar e instalar o **Eksctl**. Ambos são interfaces de linha de comandos (CLI) que foram utilizados no **PowerShell**, sendo o **Kubectl** para manipulação dos comandos do Kubernetes e o **Eksctl**, uma outra cli, além da **AWS CLI**, para manipulação dos recursos do serviços **Amazon EKS**. O arquivo `config` e o diretório `.kube` também foram criados na maquina física, pois era este arquivo responsável por conectar a cli do Kubernetes com softwares executadores de clusters Kubernetes, sendo neste caso conectado ao serviço **Amazon EKS**, pois era ele o responsável por executar o cluster nesta aula.

Após essa parte preeliminar, com o script de criação, o projeto foi iniciado com a elaboração de duas roles no serviço **AWS IAM**, sendo uma vinculada ao cluster construído no serviço **Amazon EKS** e a outra vinculada as instâncias criadas no node group deste cluster. Na primeira role foi anexada a policy `AmazonEKSClusterPolicy`, permitindo a interação com recursos do **Amazon EKS** de clusters, enquanto na segunda role foram anexadas as três seguintes polices: `AmazonEKS_CNI_Policy`, `AmazonEKSWorkerNodePolicy` e `AmazonEC2ContainerRegistryReadOnly`. A policy `AmazonEKS_CNI_Policy` forneceu as permissões necessárias para o **Amazon EKS** gerenciar a configuração de rede das instâncias no cluster Kubernetes, configurando o `Container Networking Interface (CNI)` nas instâncias **Amazon EC2**. A policy `AmazonEKSWorkerNodePolicy` concedeu as permissões necessárias para que as instâncias possam se registrar no cluster do **Amazon EKS**, interagir com o serviço e executar containers. Já a policy `AmazonEC2ContainerRegistryReadOnly` concedeu permissões de leitura apenas para o **Amazon Elastic Container Registry (ECR)**, permitindo que as instâncias EC2 lessem (pull) imagens de container do **Amazon ECR**, mas não gravassem (push). As imagens 30 e 31 evidenciam a criação dessas duas roles com suas respectivas polices. Observando que todas as polices utilizadas já existem na cloud da **AWS**.

<div align="Center"><figure>
    <img src="./0-aux/img30.PNG" alt="img30"><br>
    <figcaption>Imagem 30.</figcaption>
</figure></div><br>

<div align="Center"><figure>
    <img src="./0-aux/img31.PNG" alt="img31"><br>
    <figcaption>Imagem 31.</figcaption>
</figure></div><br>

Com as roles criadas, foi realizado o provisionamento da infraestrutura de rede através do serviço de infraestrutura como código (IaC) **AWS Cloud Formation**. Uma stack foi criada indicando um arquivo formato **YAML** como template. Este arquivo estava armazenado em um bucket do serviço **Amazon S3** do professor do curso. O próprio **AWS Cloud Formation** possui uma opção para indicar como template um arquivo em um bucket do serviço **Amazon S3**. Este arquivo, cujo nome é o [amazon-eks-vpc-private-subnets.yaml](./automation/resources/amazon-eks-vpc-private-subnets.yaml), também foi baixado e armazenado na sub-pasta `resources`, porém não foi utilizado por aqui.

Após esta execução, foi aguardado cerca de 40 segundos para que a infraestrutura de rede fosse toda provisionada através do arquivo fornecido pelo professor do curso. Esta infraestrutura conteve os seguintes recursos: sub-redes, Internet Gateway, NAT Gateway, Route Tables com rotas já desenvolvidas, Elastic IP e Security Group com regras de liberação de portas já estabelecidas, todos esses recursos foram vistos na etapa um desta aula.

Na sequência, com o comando abaixo, foi realizado a criação do cluster **Kubernetes** no serviço **Amazon EKS**. Observe que foi necessário extrair os Ids das quatro sub-redes criadas na VPC, sendo duas públicas e duas privadas. Também foi necessário o Id do Security Group desta mesma VPC e o **Amazon Resource Name (ARN)** da role com permissões para o **Amazon EKS**.

```PowerShell
aws eks create-cluster --name $clusterName --role-arn $arnRole --resources-vpc-config "subnetIds=$subnetPub1Id,$subnetPub2Id,$subnetPriv1Id,$subnetPriv2Id,securityGroupIds=$securityGroupId" --no-cli-pager
```

Novamente, foi aguardado alguns segundos para que o cluster terminasse de ser construído. Utilizando agora comandos tanto da **AWS CLI** e da **Eksctl** foi provisionado um node group para esse cluster com dois nós cujo tipo de instância foi a `t3.medium`, lembrando que esta instância não está dentro do plano **Free Tier**, portanto custos foram gerados. O comando executado foi o abaixo. Observe que foi necessário definir vários parâmetros, entre eles, as sub-redes privadas da VPC, a **ARN** da segunda role, a região, pois como estava sendo utilizado outra cli, a região não estava definada por padrão.

```PowerShell
eksctl create nodegroup --cluster $clusterName --name $nodeGroupName --nodes 2 --nodes-min 2 --nodes-max 3 --node-ami $instanceTypeCash --node-type $instanceType --spot false --node-volume-size 20 --cfn-role-arn $arnRole --subnet-ids $subnetPriv1Id,$subnetPriv2Id --region $region
```

Agora, com toda a infraestrutura pronta chegou a hora de fazer o deploy da aplicação. Como dito anteriormente, a aplicação utilizada foi a mesma da etapa 2 da aula 2, cujo nome foi `kube-news`. Diferentemente da execução das aulas 1 e 2, onde a pasta do projeto foi baixada do repositório do **GitHub** do professor direto para instância provisionada na cloud, desta vez a pasta do projeto [imersao-devops-cloud-02-main](./imersao-devops-cloud-02-main/) foi baixado do **GitHub** para a pasta deste curso. Na pasta do projeto, duas sub-pastas eram contidas, sendo uma do projeto da aula 1 (`conversao-temperatura`) e a outra do projeto da aula 2 etapa 2 (`kube-news`). Na sub-pasta do projeto da aula 2 foi criada a sub-pasta `kube-news`, onde foi feito uma cópia do arquivo de manifesto **YAML** utilizado na aula 2 etapa 2, cujo nome era `deployment2.yaml` e estava na sub-pasta `resource`. Neste arquivo copiado foi necessário realizar uma alteração, para utilização do recurso do **Load Balancer** já que agora estava sendo utilizado no ambiente de cloud. Então, no `service` da aplicação o type foi alterado de `NodePort` para `Load Balancer` e a opção `nodeport` foi comentada. Todos esses procedimentos foram realizados por fora do script de criação, pois foi decidido que não seria uma etapa recorrente, ou seja, não seria baixado e excluído pelos scripts de automação, já seria arquivos fixos e versionados para **GitHub**. Dessa forma, esse procedimento não era necessário realizar novamente quando fosse executar o projeto, podendo pular para próxima etapa e fazer o deploy do arquivo de manifesto.

Continuando no script de criação, alguns segundos foram aguardados para que o processo de construção do node group tivesse sido finalizado. Então com o comando `aws eks update-kubeconfig --name $clusterName` o arquivo configuração do **Kubectl** era configurado com o serviço **Amazon EKS**, assim os comandos para gerenciamento e manipulação do cluster **Kubernetes** poderam ser executados pelo **PowerShell** da maquina física **Windows** através do **Kubectl**. Em seguida, foi executado o comando `kubectl get nodes` para verificar os nós do cluster, que no caso eram dois. Então foi alterado para o diretório do arquivo de manifesto **YAML** com o comando `Set-Location $projectPath/kube-news/k8s` e executado o deploy com o comando `kubectl apply -f deployment2.yaml`.


<a name="item04"><h4>Aula 4 - Github Actions - Eficiência em entregas automatizadas</h4></a>[Back to summary](#item0)




$arnRole = aws iam list-roles --query "Roles[?RoleName=='curso081RoleEc2'].Arn" --output text
$vpcId = aws ec2 describe-vpcs --filters "Name=tag:Name,Values=curso081Stack-VPC" --query "Vpcs[].VpcId" --output text
$subnetPub1Id = aws ec2 describe-subnets --filters "Name=tag:Name,Values=$subnetPub1NameProf" "Name=vpc-id,Values=$vpcId" --query "Subnets[].Tags[].Value" --output text
$subnetPub2Id = aws ec2 describe-subnets --filters "Name=tag:Name,Values=$subnetPub2NameProf" "Name=vpc-id,Values=$vpcId" --query "Subnets[].Tags[].Value" --output text


$subnetPriv1Id = aws ec2 describe-subnets --filters "Name=tag:Name,Values=$subnetPriv1NameProf" "Name=vpc-id,Values=$vpcId" --query "Subnets[].Tags[].Value" --output text
$subnetPriv2Id = aws ec2 describe-subnets --filters "Name=tag:Name,Values=$subnetPriv2NameProf" "Name=vpc-id,Values=$vpcId" --query "Subnets[].Tags[].Value" --output text
$securityGroupId = aws ec2 describe-security-groups --filters "Name=tag:$securityGroupKeyProf,Values=$securityGroupNameProf" "Name=vpc-id,Values=$vpcId" --query "SecurityGroups[].GroupId[]" --output text


aws ec2 describe-subnets --filters "Name=tag:Name,Values=curso081Stack-PublicSubnet02" "Name=vpc-id,Values=vpc-0a62f3765cf5fb444" --query "Subnets[].SubnetId" --output text
aws ec2 describe-subnets --filters "Name=tag:Name,Values=curso081Stack-PublicSubnet02" "Name=vpc-id,Values=vpc-0a62f3765cf5fb444" --query "Subnets[].SubnetId" --output text


eksctl get nodegroup --cluster curso081Cluster --region $region

$subnetPriv1Id = aws ec2 describe-subnets --filters "Name=tag:Name,Values=curso081Stack-PrivateSubnet01" "Name=vpc-id,Values=vpc-0e42f1359af396268" --query "Subnets[].Tags[?Value=='curso081Stack-PrivateSubnet01'].Value" --output text

$subnetPriv1Id = aws ec2 describe-subnets --filters "Name=vpc-id,Values=vpc-0e42f1359af396268" --query "Subnets[].Tags[?Value=='curso081Stack-PrivateSubnet01'].Value" --output text
$subnetPriv2Id = aws ec2 describe-subnets --filters "Name=vpc-id,Values=vpc-0e42f1359af396268" --query "Subnets[].Tags[?Value=='curso081Stack-PrivateSubnet02'].Value" --output text



eksctl create nodegroup --cluster curso081Cluster --name curso081NodeGroup --nodes 2 --nodes-min 2 --nodes-max 3 --node-ami ami-0dbc3d7bc646e8516 --node-type t3.medium --node-volume-size 20 --cfn-role-arn arn:aws:iam::005354053245:role/curso081RoleEc2 --subnet-ids curso081Stack-PrivateSubnet01,curso081Stack-PrivateSubnet02 --region us-east-1





aws ec2 describe-vpcs --filters "Name=tag:Name,Values=$vpcName" --query "Vpcs[].VpcId" --output text
vpc-0e42f1359af396268


aws ec2 describe-subnets --filters "Name=tag:Name,Values=curso081Stack-PrivateSubnet01" "Name=vpc-id,Values=vpc-0e42f1359af396268" --query "Subnets[].SubnetId" --output text

$subnetPriv1Id = aws ec2 describe-subnets --filters "Name=tag:Name,Values=curso081Stack-PrivateSubnet01" "Name=vpc-id,Values=vpc-0e42f1359af396268" --query "Subnets[].SubnetId" --output text
$subnetPriv2Id = aws ec2 describe-subnets --filters "Name=tag:Name,Values=curso081Stack-PrivateSubnet02" "Name=vpc-id,Values=vpc-0e42f1359af396268" --query "Subnets[].SubnetId" --output text



aws eks create-nodegroup --cluster-name curso081Cluster --nodegroup-name curso081NodeGroup --subnets 
  --instance-types t3.medium --ami-type AL2_x86_64 --disk-size 20 --scaling-config minSize=2,maxSize=3,desiredSize=2 --tags KeyName1=curso081Ec2Node1
KeyName2=curso081Ec2Node2 --node-role arn:aws:iam::005354053245:role/curso081RoleEc2

aws eks describe-nodegroup --cluster-name curso081Cluster --nodegroup-name curso081NodeGroup --query "nodegroup.nodegroupName" --region us-east-1 --output text

aws eks describe-cluster --name curso081Cluster 

aws eks describe-nodegroup --cluster-name $clusterName --nodegroup-name $nodeGroupName --query "nodegroup.nodegroupName"