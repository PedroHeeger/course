# Imersão Docker e Kubernetes 1   <img src="../curso_116/0-aux/logo_course.png" alt="curso_116" width="auto" height="45">

### Repository: [course](../../../../)
### Platform: <a href="../../">fabricio_veronez   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/plataforma/fabricio_veronez.png" alt="fabricio_veronez" width="auto" height="25"></a>
### Software/Subject: <a href="../">devops   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/content/devops.png" alt="devops" width="auto" height="25"></a>
### Course: <a href="./">curso_116 (Imersão Docker e Kubernetes 1)   <img src="../curso_116/0-aux/logo_course.png" alt="curso_116" width="auto" height="25"></a>

---

### Theme:
- DevOps

### Used Tools:
- Operating System (OS): 
  - Linux   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/linux/linux-original.svg" alt="linux" width="auto" height="25">
  - Windows 11   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/windows11.png" alt="windows11" width="auto" height="25">
- Linux Distribution: 
  - Ubuntu   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/ubuntu/ubuntu-plain.svg" alt="ubuntu" width="auto" height="25">
- Cloud:
  - Amazon Web Services (AWS)   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/amazonwebservices/amazonwebservices-original.svg" alt="aws" width="auto" height="25">
- Cloud Services:
  - Amazon Elastic Compute Cloud (EC2)   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/cloud/aws_ec2.svg" alt="aws_ec2" width="auto" height="25">
  - Amazon Elastic Container Registry (ECR)   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/cloud/aws_ecr.svg" alt="aws_ecr" width="auto" height="25">
  - Google Drive <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/google_drive.png" alt="google_drive" width="auto" height="25">
- Containerization: 
  - Docker   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/docker/docker-original.svg" alt="docker" width="auto" height="25">
- Cluster Management Software:
  - Kubernetes   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/kubernetes/kubernetes-plain.svg" alt="kubernetes" width="auto" height="25">
  - K3D   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/kubernetes_k3d.png" alt="kubernetes_k3d" width="auto" height="25">
- Language:
  - HTML   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/html5/html5-original.svg" alt="html" width="auto" height="25">
  - Markdown   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/markdown/markdown-original.svg" alt="markdown" width="auto" height="25">
  - Node.js   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/nodejs/nodejs-original.svg" alt="nodejs" width="auto" height="25">
  - YAML   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/yaml.png" alt="yaml" width="auto" height="25">
- Integrated Development Environment (IDE) and Text Editor:
  - Nano   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/nano.png" alt="nano" width="auto" height="25">
  - Visual Studio Code (VS Code)   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/vscode/vscode-original.svg" alt="vscode" width="auto" height="25">
- Versioning: 
  - Git   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/git/git-original.svg" alt="git" width="auto" height="25">
- Repository:
  - Docker Hub   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/docker_hub.png" alt="docker_hub" width="auto" height="25">
  - Docker Registry   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/docker_registry.png" alt="docker_registry" width="auto" height="25">
  - GitHub   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/github/github-original.svg" alt="github" width="auto" height="25">
- Command Line Interpreter (CLI):
  - AWS Command Line Interface (CLI)   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/cloud/aws_cli.svg" alt="aws_cli" width="auto" height="25">
  - Bash e Sh   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/bash/bash-original.svg" alt="bash_sh" width="auto" height="25">
  - Windows PowerShell   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/windows_power_shell.png" alt="windows_power_shell" width="auto" height="25">
- Server and Databases:
  - Nginx   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/nginx/nginx-original.svg" alt="nginx" width="auto" height="25">
  - PostgreSQL 
- Database Administration Tool:
  - DBeaver   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/dbeaver.png" alt="dbeaver" width="auto" height="25">
- Tools:
  - Advanced Package Tool (Apt)   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/apt.png" alt="apt" width="auto" height="25">
  - Advanced Package Tool (Apt-Get)   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/apt-get.jpg" alt="apt-get" width="auto" height="25">
  - Curl   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/curl.png" alt="curl" width="auto" height="25">
  - Node Package Manager (npm)   <img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/npm/npm-original-wordmark.svg" alt="npm" width="auto" height="25">
  - Unrar   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/unrar.png" alt="unrar" width="auto" height="25">
  - Wget   <img src="https://github.com/PedroHeeger/main/blob/main/0-aux/logos/software/wget.webp" alt="wget" width="auto" height="25">
  
---

<a name="item0"><h3>Course Strcuture:</h3></a>
1. <a href="#item01">Aula 1 - Fundamentos e primeiros passos com Docker</a><br>
2. <a href="#item02">Aula 2 - Imagem Docker e sua aplicação 100% em containers com Docker Compose</a><br>
3. <a href="#item03">Aula 3 - Kubernetes: deploy eficiente das aplicações + Oportunidade de se tornar Expert</a><br>

---

### Objective:
O objetivo desse curso foi introduzir algumas duas das principais ferramentas da área de **DevOps**, que são os softwares **Docker** e **Kubernetes**. No curso foram desenvolvidos alguns projetos básicos apenas para explicar sobre cada software e um projeto principal unindo as duas tecnologias. Este projeto principal consistiu em uma aplicação web containerizada de um microblog (portal de notícias) escrito em **Node.js** que tem como persistência dos dados em um container com imagem do banco **PostgreSQL**.

### Structure:
A estrutura do projeto (Imagem 01) é formada por:
- A pasta `automation` com os arquivos de scripts em **PowerShell** de automação (`criacao`, `exclusao` e `variaveis`) .
- A pasta [project](./project/) armazenando a pasta do repositório do **GitHub** do professor do curso que foi baixada para a maquina física.
- A pasta [resources](./resources/) que continha os arquivos de recursos necessários para execução dos projetos do curso. Esta pasta foi sub-dividido nos seguintes diretórios para uma melhor organização dos arquivos: 
    - A sub-pasta [bash](./automation/resources/bash/) contendo os arquivos em **Bash** utilizados no decorrero do curso;
    - A sub-pasta [docker](./automation/resources/docker/) contendo o arquivo `Dockerfile` utilizado na aula 2; 
    - A sub-pasta [others](./automation/resources/others/) contendo outros arquivos.
- A pasta [secrets](./secrets/) era onde estavam os dados pessoais sensíveis e por isso não foi versionada para o **GitHub**. Ela foi ramificada nos seguintes diretórios:
    - A sub-pasta `.aws` armazenando os arquivos de credenciais e configuração do usuário do IAM `PedroHeegerAdmin` na **AWS CLI**.
    - A sub-pasta `.docker` que armazenava o arquivo configuração com a autenticação no usuário da minha conta no **Docker Hub**
    - A sub-pasta `keyPair` contendo arquivos par de chaves `.pem` gerado para realização de acesso remoto em maquinas virtuais instanciadas na cloud da **AWS** durante o curso.
- A pasta `0-aux`, pasta auxiliar com imagens utilizadas na construção desse arquivo de README. 

<div align="Center"><figure>
    <img src="./0-aux/img01.png" alt="img01"><br>
    <figcaption>Imagem 01.</figcaption>
</figure></div><br>

### Development:
O curso, realizado em três aulas, teve a maior parte do seu desenvolvimento executado na cloud da **AWS** no intuito de evitar instalações na maquina física **Windows**. Assim, o que foi realizado de forma local, na maquina física, seria realizado em uma maquina virtual em ambiente de cloud. Para isso, a maquina física já possuia instalada a interface de linha de comando da AWS (**AWS CLI**), configurada com acesso do usuário do IAM (`PedroHeegerAdmin`) que era o administrador da conta, sendo utilizada através do **PowerShell** para interagir com os serviços da **AWS**. O desenvolvimento do curso foi fragmentado em etapas, sendo que em uma etapa poderia ter mais de uma aula e uma aula poderia ter mais de uma etapa. Como forma de automatizar o máximo do curso, cada etapa correspondia a um script **PowerShell** elaborado no arquivo de criação de nome [creation.ps1](./automation/creation.ps1) e a um script **Bash** desenvolvido no arquivo de user data [udCurso116.sh](./resources/udCurso116.sh) executado dentro da instância que seria criada como ambiente de execução. Em cada script do arquivo `creation.ps1`, era utilizado uma estrutura de condicional que aguardava uma entrada do usuário para determinar se a etapa referente ao script seria executada ou não. Já os scripts do arquivo `udCurso116.sh` era executado sequencialmente logo após a maquina ser instanciada na **AWS**. A seguir são listadas as etapas destes arquivos:

- ETAPA 1 (AULAS 1 E 2): Nesta etapa, com o arquivo `creation.ps1`, foi criada uma instância **Linux Ubuntu** no serviço **Amazon Elastic Compute Cloud (EC2)** para funcionar como ambiente de execução, pois o que fosse realizado localmente pelo curso, seria realizado nesta maquina na cloud **AWS**. Essa instância precisou ainda da criação de um arquivo par de chaves vinculado a ela, para possibilitar a realização de acesso remoto. A liberação da porta `8080` do grupo de segurança vinculado a esta instância, pois seria a porta onde as aplicações iriam rodar tanto no container como na própria instância. A liberação da porta `5432`, porta padrão do banco de dados **PostgreSQL**, para permitir o acesso ao banco de dados através do software **Dbeaver**. A porta `22` para realização do acesso remoto já estava liberada. Além disso, foi realizado a transferência de arquivos e pastas da maquina física para a instância, para configuração dos softwares que seriam instalados **AWS CLI** e **Docker** e os arquivos referente a execução dos projetos. Também foi vinculado a instância o arquivo user data de nome `udCurso116.sh` que executaria as etapas dentro da instância (ambiente de execução), sendo neste caso quatro etapas que são listadas abaixo.
    - ETAPA 1: INSTALACOES DE SOFTWARES: Nesta primeira etapa foram realizadas instalações de várias softwares que seriam utilizados ao longo do curso. A sequência foi ferramentas básicas do **Linux** (**Nano**, **Wget**, **Curl** e **Unzip**), **Git**, **ZSH**, **Oh My ZShell** e três plugins dele, **AWS CLI**, **Docker**, **K3D**, **Kubectl**, **NPM** e **Node.js**.
    - ETAPA 2: BAIXANDO OS ARQUIVOS DOS PROJETOS DO GITHUB: Na etapa dois, a pasta do repositório do **GitHub** do professor do curso foi baixada e sub-pastas necessárias para os projetos da aula 2 e 3 foram criadas.


    - ETAPA 3: DOCKER PROJETO 1 (AULA 1): Nesta etapa foi realizado o projeto número 1 do curso que era referente a aula 1 sobre o software **Docker**.
    - ETAPA 4: KUBERNETES PROJETO 2 (AULA 2): Na etapa de número 6 foi executado o segundo projeto do curso, cujo software em questão foi o **Kubernetes**.
    - ETAPA 5: KUBERNETES PROJETO 3 (AULA 2): Na última etapa deste arquivo em **Bash** na instância do EC2 da **AWS** foi realizado o projeto de número 3 que era o projeto principal, utilizando os dois softwares **Docker** e **Kubernetes**. A partir daí, a instância pôde ser encerrada, concluindo as aulas 1 e 2 (projetos 1, 2 e 3) e retornado para o arquivo de script em **PowerShell** na maquina física para seguir para etapa 2 dele.

Adicionalmente ao arquivo de criação, em **PowerShell** também existia o arquivo de [exclusion.ps1](./automation/exclusion.ps1) que fazia o processo inverso, removendo o que tinha sido construído nas etapas e o arquivo [variable.ps1](./automation/variable.ps1) que continha todas as variáveis utilizadas por este dois arquivos. Para o arquivo de variáveis ser utilizado neste dois arquivos foi necessário declará-los em cada um deles. Além das estruturas de condição `if else` presentes nesses dois arquivos (`creation.ps1` e `exclusion.ps1`), a maior parte dos comandos executados durante as etapas foram precedidos por comandos de verificação, também através de estruturas de condicionais, determinando se o serviço ou recurso já havia sido criado ou excluído na cloud **AWS**. Todos os três arquivos foram armazenados no diretório [automation](./automation/).

<a name="item01"><h4>Aula 1 - Fundamentos e primeiros passos com Docker</h4></a>[Back to summary](#item0)

Nesta primeira aula do curso, foi realizado uma introdução sobre o software **Docker**, explicando suas vantagens, as tecnologias principais que permitem trabalhar com essa ferramenta, os conceitos de *container* e *imagem*, a arquitetura do **Docker**, as vantagens de utilizá-lo e como está o mercado em relação ao **Docker**. Também foi iniciado uma demonstração de como utilizá-lo, apresentando comandos básicos.

O mercado está cada vez mais competitivo, entregas mais rápidas e com qualidade são uma necessidade, a adoção da arquitetura de microsserviços se tornou um requisito, diversas tecnologias estão sendo utilizadas no mesmo projeto, as empresas cada vez mais estão implementando soluções de DevOps e Cloud. Por conta disso tudo, o uso de containers se tornou um padrão de mercado e para trabalhar com containers, **Docker** e **Kubernetes** são as ferramentas mais adotadas atualmente. Um profissional que domina o **Docker**, consegue ter mais velocidade e mais independência na hora de desenvolver, tem mais facilidade em trabalhar com tecnologias complexas, consegue participar de projetos mais desafiadores e, por consequência, consegue as melhores oportunidades.

A utilização do **Docker** tem como vantagens: melhorar o aproveitamento de recurso da maquina, ter velocidade pra desenvolver e executar aplicações, simplificaçar o deploy, ter portabilidade entre ambientes e similaridade entre ambiente dev e de produção e garantir a idempotência, que é garantir que sempre que for executado um container, ele terá o mesmo comportamento, não importa em qual ambiente esteja sendo executado desde que esse ambiente execute containers.

O **Linux** é quem tem a tecnologia, os recursos no kernel para poder trabalhar com containers. As três principais tecnologias para se trabalhar com containers são: *Namespace*, *Cgroup* e *Overlay File System (OverlayFS)*. O *Namespace* é a tecnologia do kernel que possibilita executar processos dentro de processos no **Linux** e dessa forma é que se consegue criar o isolamento de processos. O *Cgroups* entra para limitar a quantidade de recursos para cada processo, então é possível limitar cpu, limitar memória, contribuindo para o isolamento de processos. Já o *Overlay File System* é uma tecnologia que faz com que consiga juntar duas camadas de arquivos de file system, ou seja, pegar dois diretórios com suas estruturas de arquivos dentro deles e uní-los. Essa tecnologia é utilizada na hora de construir imagens de container **Docker**.

A *imagem* é um objeto do **Docker** quer serve de template para criação dos containers. Então a partir de uma única imagem é possível criar vários containers. Nela vai ser colocado tudo que é necessário para criar e executar a aplicação. A imagem, nada mais é, que um conjunto de arquivos criando um File System (Sistema de arquivos). Os *containers* por sua vez, são instâncias das imagens. Eles são objetos efêmeros, que após terem cumprido seu papel podem ser removidos.

Com relação a arquitetura do **Docker**, o primeiro elemento e o principal é o *Docker Daemon*. No *Docker Daemon*, é onde é executado e gerenciado todos os objetos do **Docker** (*imagens*, *containers* e *networks*), todos os componentes do **Docker** vão está dentro do *Docker Daemon* e é nele que vai ser executado e gerenciado os containers. Mas para trabalhar com o *Docker Daemon*, tem que ser utilizado o *Docker Client*. O *Docker Client* é responsável por enviar as instruções e fazer a comunicação do usuário com o *Docker Daemon*. Então toda vez que um comando é executado, o *Docker Client* está sendo utilizado para comunicar com o *Docker Daemon*. Muitas vezes, é possível ter instalado o *Docker Daemon* e o *Docker Client* no mesmo ambiente, no que é chamado de *Docker Host* que é a maquina responsável por hospedar e executar o *Docker Daemon*, normalmente tem-se os dois no *Docker Host*. Mas existe a possibilidade de ter o *Docker Client* em uma maquina se comunicando com o *Docker Daemon* que está em outra maquina remota que está em um datacenter em um ambiente de cloud.

Contudo, as imagens que vão ser utilizadas como base para criar os containers, elas precisam estar em algum lugar. É ai que entra o *Docker Registry*, o *Docker Registry* é componente do **Docker** utilizado para armazenar as imagens. Junto com o conceito do *Docker Registry*, tem o **Docker Hub** que é o *Docker Registry* oficial do **Docker**, o principal serviço de *Docker Registry* hoje do mercado e que pertence a **Docker**. Ele não é o único, existem diversos outros serviços, inclusive cada Cloud Provider tem o seu serviço de Docker ou Container Registry, como ECR na **AWS** ou o ACR na **Azure**. Inclusive, é possível utilizar ferramentas como o **Harbor** para criar seu próprio Registry no seu ambiente de datacenter.

###### Demonstração

A demonstração da utilização do **Docker** foi realizado na instância **Linux Ubuntu** criada no serviço **Amazon Elastic Compute Cloud (EC2)** da **AWS**, para os projetos do curso. Essa demonstração foi executada de forma manual, após a instância estar ativa, realizando um acesso remoto à ela com o software **OpenSSH**. Essa instância foi criada através do arquivo de criação em **PowerShell** explicado no sub-tópico `Automatização` logo abaixo. A demonstração foi iniciada com o comando `docker container run hello-world` para baixar e executar um container de exemplo que exibiu a mensagem `hello-world` e depois se encerrou. Com o parâmetro `--name meucontainer1` foi possível executá-lo novamente dando um nome para o container. Com o comando `docker ls` foi listado todos os containers em execução, porém como esses dois criados já tinham sidos encerrados, pois depois que eles exibiram a mensagem eles se encerraram, nenhum container foi exibido. Para listar todos os containers tanto os ativos e os inativos foi acrescentado o parâmetro `-a (--all)`. Para remover os containers inativos utilizou-se o comando `docker rm` com o ID ou nome do container. Quando utilizado com o ID não foi necessário por o ID todo, se não houvesee IDs com digitos inicias iguais, bastava por os dois primeiros IDs que ele identificava e deletava. Caso tivesse containers com IDs com dois dígitos iguais, só era acrescentar mais um dígito até que eles se diferenciassem. Com o comando `docker container --name meucontainer2 run --rm` e a imagem `hello-world` foi construído um container auto-destrutivo, no qual após ele terminasse a execução, ao invés de ficar encerrado, ele era removido.

Dando sequencia, com o comando `docker container run --name container2 -it ubuntu /bin/bash` era criado um container nomeado e com a imagem do **Linux Ubuntu**. O parâmetro `-i (--interactive)` mantinha o o stdin aberto para que fosse possível interagir com o processo executado no container. O *Standard Input (stdin)* é o canal de entrada padrão, sendo uma das três "streams" padrão associadas a um processo em sistemas operacionais Unix-like (como o **Linux**), juntamente com *stdout (saída padrão)* e *stderr (saída de erro padrão)*. Ou seja, com o stdin pôde ser fornecido dados de entrada para o processo dentro do container. Já o parâmetro `-t (--tty)` alocava um interface de terminal, pseudo-TTY (Terminal Teletype), dentro do ambiente do container para que fosse possível enviar e receber sinais de controle para os processos do container. A opção `/bin/bash` foi apenas um comando **Linux** sendo executado, que neste caso, era o caminho até o binário do **Bash**, ou simplesmente o link simbólico `bash`, para iniciar um shell bash. Com o pseudo-terminal alocado, foi possível interagir com os processos desse container que era o **Linux Ubuntu**. Com comando `cat /ect/os-release` foi visualizada as informações do sistema operacional que esse container utilizava. Já com o comando `apt-get update -y` e `apt-get install -y curl` foi atualizado os pacotes do sistema e baixado o software **Curl**. Com o **Curl** foi executado `curl -IL www.google.com` para verificar se o acesso ao site funcionava. Após isso, foi cancelado o processo em execução no terminal local, para voltar ao ambiente local e encerrar o pseudo-terminal do ambiente de container criado.

Com o comando `docker container run -it alpine /bin/bash` foi desenvolvido um outro container que utilizava a imagem do **Linux Alpine** que é uma imagem **Linux** bem mais leve que as outras distruibuições. Também foi alocado o tty e permitido a interatividade, executando novamente um shell bash. Em seguida, foi encerrado o processo e voltado para o terminal local. Com o comando `docker container ls -aq` era listado apenas os IDs de todos os containers (ativos e inativos). O parâmetro `-q (--quiet)` exibiu apenas os IDs dos containers. Com uma combinação de dois comandos `docker container rm $(docker container ls -aq)` foi listado todos os IDs de todos os containers (ativos e inativos) e executado a exclusão deles.

Agora, com o comando `docker container run --name meucontainer2 nginx` foi criado um novo container com imagem do servidor web **Nginx**, porém esse processo estava sendo executado em primeiro plano do terminal, o que deixa o terminal preso. Para liberar o terminal, teve que utilizar o parâmetro `-d (--detach)` para enviar o terminal para segundo plano ou background. Assim, o container continuava em execução (ativo) e o terminal local estava livre para executar novos comandos. Com o comando `docker logs` e o nome ou ID do container foi possível visualizar os logs do container especificado. Já para executar um comando no container pelo terminal local utilizou-se `docker container exec`, nome ou ID do container e o comando que quisesse executar, no caso foi o `ls`. Caso fosse necessário permitir a interatividade e alocar uma tty no ambiente de container com ele em execução era utilizado o comando `docker container exec -it meucontainer2 /bin/bash`, iniciando o shell bash.

##### Automatização

Até esse momento, os comandos tinham sido executados de forma manual através do acesso remoto realizado na instância criada. Agora vai ser explicado como foi feito toda a parte automatizada do curso, como montagem do ambiente de execução e a construção e implantação dos projetos. A construção do ambiente e deploy dos projetos foi efetuada inicialmente através do arquivo de criação [creation.ps1](./automation/creation.ps1) em **PowerShell**, utilizando o **AWS CLI** previamente instalado e com o usuário do IAM administrador da minha conta **AWS** (`PedroHeegerAdmin`) já configurado na maquina física. Assim, foi possível interagir com os serviços da **AWS**. A primeira etapa e única desse arquivo de criação executou alguns scripts, sendo o primeiro deles a construção de um par de chaves na **AWS** que seria vinculado com a instância para possibilitar a autenticação durante um acesso remoto da maquina física à instância. Ao mesmo tempo que o par de chaves era criado, foi realizado um redirecionamento da chave privada para armazená-la em arquivo de nome `keyPairCurso116.pem` que era também criado neste momento no diretório `secrets/keyPair`.

Após a geração do par de chaves, a instância do tipo `t3.medium` era construída, pois para executar cluster **Kubernetes** era necessário do poder computacional que esse tipo de instância possibilitava. Foram definidos as sub-redes e o grupo de segurança vinculados a instância como os padrões da VPC padrão da região (`us-east-1`). Também foram definidos uma tag de noma para instância e a imagem de maquina que ela utilizaria, ou seja, o sistema operacional, que foi um **Linux Ubuntu**. Por fim, como parâmetro `user data` foi passado o arquivo de script em **Bash** [udCurso116.sh](./resources/bash/udCurso116.sh). Esse arquivo foi o responsável por realizar a automação dentro da instância, com instalações e configurações de software, organização e implantação dos projetos do curso. Mais a frente será detalhado as etapas dele. 

O próximo script do arquivo `creation.ps1` criou duas regras de entrada no grupo de segurança padrão que era o utilizado pela instância, liberando as entradas nas portas `8080` e `5432` que eram referentes a aplicação web dos projetos e o banco de dados **PostgreSQL**. Foi permitindo acesso de qualquer faixa de IP. Contudo, isso não é uma prática recomendada, mas como neste caso foi para fins didáticos, foi realizado. Nesse momento, foi aguardado alguns segundos antes de executar o próximo script, isso porque, o arquivo de script em **Bash** estava em execução na instância e era necessário que os softwares **AWS CLI** e **Docker** já tivessem sido instalados por ele.

Depois de alguns segundos de espera, foi exibido o endereço para acesso a aplicação pelo navegador da maquina física. Esse endereço era formado pelo IP público da instância, onde a aplicação iria ser executada, e a porta, que no caso para aplicação web era `8080`, enquanto para o banco de dados era `5432`. Também foi exibido o comando para acesso remoto da maquina física a instância com o software **OpenSSH**. Em seguida, estruturas condicionais verificavam se alguns arquivos e pastas que desejava-se transferir para instância, já existia nela, caso não existisse, esses arquivos eram transferidos da maquina física para instância utilizando também o **OpenSSH**. O primeiro a ser verificado foi a pasta `.aws` que continha os arquivos `.config` e `.credentials`, esses arquivos realizavam o login do usuário do IAM worker da minha conta da **AWS** (`PedroHeegerWorker`) no **AWS CLI** instalado na instância. O segundo foi a pasta `.docker` que continha o arquivo `config.json` com a autenticação para acesso a minha conta no **Docker Hub** pelo **Docker** instalado na instância.

Um outro container com a mesma imagem foi executado com o comando `docker container run -d -p 8080:80 nginx`, neste caso, foi utilizado o parâmetro `-p (--pubish)` para realizar um bind de portas, ou seja, direcionar o tráfego da porta `8080` do host, que no caso era a instância do EC2, para a porta `80` do container. Assim, era possível acessar a aplicação em execução no container, que era um servidor web **Nginx**, no IP público da instância concatenado com `:` e a porta `8080`. Na imagem 02 é exibido a aplicação containerizada, no caso o servidor web, sendo acessada pelo navegador da maquina física.

<div align="Center"><figure>
    <img src="./0-aux/img02.png" alt="img02"><br>
    <figcaption>Imagem 02.</figcaption>
</figure></div><br>

##### Bash

Após a instância ser criada, o arquivo de script **Bash** passado como parâmetro `user data` era executado dentro da instância. Esse arquivo também foi dividido em etapas, sendo cinco etapas realizadas. A primeira delas foi a instalação de todos os softwares, sendo alguns básicos e outros necessários para os projetos do curso. Os primeiros softwares instalados foram as ferramentas do **Linux**: **Nano**, **Wget**, **Curl** e **Unzip**. Na sequência foi instalado o **Git** e logo depois o **ZShell**, definido ele como shell padrão para esse usuário, que no caso era `ubuntu`, da instância. Com o **ZShell** foi instalado o **Oh My ZShell** e três de seus plugins: `Power Level`, `Auto Suggestions` e `Syntax Highlighting`. Após a configuração do shell, foi instalada a **AWS CLI** para a instância interagir com os serviços da **AWS**, caso necessário. Como o arquivo de configuração do usuário do IAM worker da minha conta da **AWS** já era enviado pelo arquivo de criação com o software **OpenSSH**, este **AWS CLI** já era vinculado a esse usuário. Em seguida, foi realizada a instalação e configuração do software **Docker** utilizado nas aulas do curso. O **K3D** e o **Kubectl** foram instalados logo em seguida, esses dois permitiriam trabalhar com o **Kubernetes**, assunto da aula 2 e 3 do curso. Por fim, foram instalados os softwares **NPM** e **Node.js** para executar, primeiramente, a aplicação web sem a utilização de containers, rodando direto na instância.

Na segunda etapa desse arquivo de script **Bash**, foi feito o clone da pasta do repositório do **GitHub** do professor do curso para a instância, armazenando dentro do diretório do usuário `/home/ubuntu`. Também foi alterado o usuário e grupo desta pasta para o usuário `ubuntu`, que era o usuário que realziava os acessos remotos.







Para criar um container de banco de dados, foi utilizado o comando `docker container run -d -p 5432:5432 postgres` que utilizou os mesmos parâmetros do anterior, apenas alterando a imagem para de um **PostgreSQL**, o bind de portas para `5432:5432` que é a padrão deste banco de dados. Contudo, após ele ser executado, ele ficou inativo. Isso ocorreu, porque o **PostgreSQL**, assim como outras imagens, necessitam que seja definido as variáveis de ambiente. Para isso, foi utilizado o comando `-e (--environment)` e passado as variáveis `POSTGRES_PASSWORD`, `POSTGRES_USER` e `POSTGRES_DB`, que eram a senha, o usuário e o nome do banco de dados. O novo comando ficou assim `docker container run -d -p 5432:5432 -e POSTGRES_PASSWORD=Pg#123 -e POSTGRES_USER=kubenews -e POSTGRES_DB=kubenews postgres`. Com o container de banco de dados aitvo, foi utilizado um software de administração de banco de dados, instalado na maquina física, que no caso foi o **Dbeaver**, utilizado para acessar esse banco do container com uma interface gráfica. No **Dbeaver** foi criada uma conexão com o container de banco de dados, informando que o host era o IP público da instância, a porta era a `5432` definida no port bind, o nome do banco, o nome do usuário e a senha foram os mesmos estabelecidos na variável de ambiente. Na imagem 03 é visualizado o banco de dados do container sendo acessado pelo **Dbeaver**. Na imagem 04 é listado na instância do EC2 os dois containers criados.

<div align="Center"><figure>
    <img src="./0-aux/img03.png" alt="img03"><br>
    <figcaption>Imagem 03.</figcaption>
</figure></div><br>

<div align="Center"><figure>
    <img src="./0-aux/img04.png" alt="img04"><br>
    <figcaption>Imagem 04.</figcaption>
</figure></div><br>


<a name="item02"><h4>Aula 2 - Imagem Docker e sua aplicação 100% em containers com Docker Compose</h4></a>[Back to summary](#item0)







<a name="item03"><h4>Aula 3 - Kubernetes: deploy eficiente das aplicações + Oportunidade de se tornar Expert</h4></a>[Back to summary](#item0)

